ollama:
  gpu:
    # -- Enable GPU integration
    enabled: true
    # -- Specify the number of GPU to 1
    number: 1
  # -- List of models to pull at container startup
  models: 
    - llama3


persistentVolume:
  enabled: true
  size: 250Gi


